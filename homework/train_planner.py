"""
Usage:
    python3 -m homework.train_planner --model mlp_planner --num_epoch 50
    python3 -m homework.train_planner --model transformer_planner --num_epoch 100
    python3 -m homework.train_planner --model cnn_planner --num_epoch 100
"""

# Some of the code is generated by using gemini using colab

import argparse
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter

from .datasets.road_dataset import load_data
from .metrics import PlannerMetric
from .models import load_model, save_model


def train(
    model_name: str,
    transform_pipeline: str = "default",
    num_epoch: int = 50,
    lr: float = 1e-3,
    batch_size: int = 128,
    num_workers: int = 4,
    log_dir: Path = Path("logs"),
):
    """
    Train a planner model.

    Args:
        model_name: name of the model to train (mlp_planner, transformer_planner, cnn_planner)
        transform_pipeline: data transformation pipeline ('default', 'state_only', 'aug')
        num_epoch: number of epochs to train
        lr: learning rate
        batch_size: batch size
        num_workers: number of data loader workers
        log_dir: directory to save logs
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    if model_name in ["mlp_planner", "transformer_planner"]:
        transform_pipeline = "state_only"
    else:
        transform_pipeline = "default"

    print(f"Loading data with transform: {transform_pipeline}")
    train_loader = load_data(
        "drive_data/train",
        transform_pipeline=transform_pipeline,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
    )
    val_loader = load_data(
        "drive_data/val",
        transform_pipeline=transform_pipeline,
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )

    print(f"Creating model: {model_name}")
    model = load_model(model_name, with_weights=False)
    model = model.to(device)

    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=5, verbose=True
    )

    log_dir = log_dir / f"{model_name}_{datetime.now().strftime('%m%d_%H%M%S')}"
    writer = SummaryWriter(log_dir)
    print(f"Logging to {log_dir}")

    best_val_error = float("inf")

    for epoch in range(num_epoch):
        model.train()
        train_metric = PlannerMetric()
        total_train_loss = 0

        for batch in train_loader:
            track_left = batch.get("track_left", None)
            track_right = batch.get("track_right", None)
            image = batch.get("image", None)
            waypoints = batch["waypoints"].to(device)
            waypoints_mask = batch["waypoints_mask"].to(device)

            if track_left is not None:
                track_left = track_left.to(device)
            if track_right is not None:
                track_right = track_right.to(device)
            if image is not None:
                image = image.to(device)

            if model_name in ["mlp_planner", "transformer_planner"]:
                pred_waypoints = model(track_left=track_left, track_right=track_right)
            else:
                pred_waypoints = model(image=image)

            mask_expanded = waypoints_mask.unsqueeze(-1)
            loss = loss_fn(pred_waypoints * mask_expanded, waypoints * mask_expanded)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()
            train_metric.add(pred_waypoints, waypoints, waypoints_mask)

        avg_train_loss = total_train_loss / len(train_loader)
        train_metrics = train_metric.compute()

        model.eval()
        val_metric = PlannerMetric()
        total_val_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                track_left = batch.get("track_left", None)
                track_right = batch.get("track_right", None)
                image = batch.get("image", None)
                waypoints = batch["waypoints"].to(device)
                waypoints_mask = batch["waypoints_mask"].to(device)

                if track_left is not None:
                    track_left = track_left.to(device)
                if track_right is not None:
                    track_right = track_right.to(device)
                if image is not None:
                    image = image.to(device)

                if model_name in ["mlp_planner", "transformer_planner"]:
                    pred_waypoints = model(track_left=track_left, track_right=track_right)
                else:
                    pred_waypoints = model(image=image)

                mask_expanded = waypoints_mask.unsqueeze(-1)
                loss = loss_fn(pred_waypoints * mask_expanded, waypoints * mask_expanded)
                total_val_loss += loss.item()
                val_metric.add(pred_waypoints, waypoints, waypoints_mask)

        avg_val_loss = total_val_loss / len(val_loader)
        val_metrics = val_metric.compute()

        scheduler.step(val_metrics["l1_error"])

        writer.add_scalar("train/loss", avg_train_loss, epoch)
        writer.add_scalar("train/l1_error", train_metrics["l1_error"], epoch)
        writer.add_scalar("train/longitudinal_error", train_metrics["longitudinal_error"], epoch)
        writer.add_scalar("train/lateral_error", train_metrics["lateral_error"], epoch)

        writer.add_scalar("val/loss", avg_val_loss, epoch)
        writer.add_scalar("val/l1_error", val_metrics["l1_error"], epoch)
        writer.add_scalar("val/longitudinal_error", val_metrics["longitudinal_error"], epoch)
        writer.add_scalar("val/lateral_error", val_metrics["lateral_error"], epoch)

        print(f"Epoch {epoch + 1}/{num_epoch}")
        print(f"  Train Loss: {avg_train_loss:.4f} | "
              f"Long: {train_metrics['longitudinal_error']:.4f} | "
              f"Lat: {train_metrics['lateral_error']:.4f}")
        print(f"  Val Loss:   {avg_val_loss:.4f} | "
              f"Long: {val_metrics['longitudinal_error']:.4f} | "
              f"Lat: {val_metrics['lateral_error']:.4f}")

        if val_metrics["l1_error"] < best_val_error:
            best_val_error = val_metrics["l1_error"]
            save_path = save_model(model)
            print(f"  âœ“ Saved best model to {save_path}")

    writer.close()
    print(f"\nTraining complete! Best val L1 error: {best_val_error:.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, 
                       choices=["mlp_planner", "transformer_planner", "cnn_planner"],
                       help="Model to train")
    parser.add_argument("--num_epoch", type=int, default=50, help="Number of epochs")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers")
    parser.add_argument("--log_dir", type=Path, default=Path("logs"), help="Log directory")

    args = parser.parse_args()

    train(
        model_name=args.model,
        num_epoch=args.num_epoch,
        lr=args.lr,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        log_dir=args.log_dir,
    )
